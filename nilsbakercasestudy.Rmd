---
title: "Nils Baker Case Study"
author: "Chaman Preet Kaur and Chris Atterbury"
date: "October 15, 2015"
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
output: 
  pdf_document:
    toc_depth: 2
    number_sections: true
    includes:
      geometry: margin=1in
      fontsize: 16pt
---

# Executive Summary
This section is to be written last and contains a short (approx 250 words)
summary of the case study as a whole.  

# Background

The financial success of a bank depends on its total holdings, which in turn depend on the size of its customer base. The case study revolves around Nils Baker, vice president of a regional retail bank in the US. Based on the feedback received from a promising customer, he develops a hypothesis that having more physical branches motivates people to open a checking account with the bank.  In order to make a strategic business plan based on this hypothesis, it needs to be proven statistically. The data available for analysis includes: total households in a particular area, total number of checking accounts with the bank in that area and the presence of a physical branch or an ATM in that area. Setting up a physical branch involves a lot of investment and there are many maintenance fees, but if it can be proven to help gain more checking accounts in turn expanding the customer base and adding to the the total holdings, it can be a profitable business move. This is what we explore here.  

# Models
## Initial Considerations
```{r echo=FALSE}
d <- read.csv("41330723.csv", header = TRUE, stringsAsFactors = FALSE)
d <- d[1:120, ] # last two rows contain no data
names(d) <- c("ID", "Total.Households", "Accounts", "Footprint")

for (i in 2:3) {
  d[[i]] <- gsub(",", "", d[[i]])
  d[[i]] <- as.numeric(d[[i]])
}
d[["Footprint"]][d[["Footprint"]] == "Outside"] <- 0
d[["Footprint"]][d[["Footprint"]] == "Inside"] <- 1
d[["Footprint"]] <- as.numeric(d[["Footprint"]])
d[["ID"]] <- as.numeric(d[["ID"]])
d[["Accts.Hsehld"]] <- d[["Accounts"]] / d[["Total.Households"]]
```  
The data that we have received to pursue Nils Baker's hypothesis includes four columns. Initially, they are _ID_, _Total.Households.in.Area_, _Households.with.Account_, and _Inside.Outside.Footprint_. First, we drop the
last two rows of the data set because they are empty and change the column names to _ID_, _Total.Households_, _Accounts_, and _Footprint_, respectively. Next, further preprocessing of the data is performed so that we have numeric data (see __Additional Codeblock 1__ in the _Appendix_ for the code used to do this).  
The first column, _ID_, is identical to the row number and, thus, will not be considered in this case study. Each row itself is a separate Metropolitan Statistical Area (MSA). For our purposes, we need only understand that each row contains a diferent geographic region. The next two columns, _Total.Households_ and _Accounts_, contain the number of total households and the number of those households that have a checking account with the bank. The last column, _Footprint_, was originally coded with either "Inside" or "Outside." A region was considered "Inside" if there was both a physical bank location and an ATM in the region. This was recoded as 1 for easier analysis of the data. A region was considered "Outside" if there was not a physical bank location and only an ATM in the region. This was recoded as 0.  
Finally, we will add a column called _Accts.Hsehld_ that takes the _Accounts_ column and divides it by the _Total.Households_ column. We may want to choose this as the response variable since this allows us to get closer to evaluating Nils Baker's hypothesis without as much interpretation of the model.  
With the data in a usable form, we can now view the correlation matrix to get an initial feel for the relationships amongst the variables.  

\begin{center} \textbf{Figure 1} \end{center}

```{r echo=FALSE}
cor(d)
```  
We can ignore the values involving `ID` since this is basically just a running counter of which number region we see. Its correlation with `Total.Households` is based on the fact that the data set's observations are organized by the number of households in the region with the largest first. We see that `Accounts` has a strong correlation with `Total.Households`, which is reasonable since we expect that regions with more households will have more accounts, just based on volume. Also, this explains the negative correlation between `Accounts` and `ID`. The numbers indicate what we would expect for `Footprint`; the correlations with the households and accounts do not show anything. That is, the locations of physical banks were not chosen based on the number of households or accounts in a region. This  correlation matrix ends up showing us that trying to predict `Accts.Hsehld` may be difficult, but hopefully combining a few features will help. Please see __Additional Figure 1__ in the _Appendix_ for the pairs plot that shows these relationships graphically.  

## Procedure
The goal in this case study, as stated by Nils Baker, is to ascertain whether or not the presence of a physical bank in a region increases the likelihood of a given household possessing a checking account. We will start with Simple Linear Regression (SLR) models, before considering more complex Multiple Linear Regression (MLR) models.  

## SLR Models
### Predicting Number of Accounts
We will start by evaluating SLR models for `Accounts`. Even though these models may be more difficult to interpret in terms of Nils Baker's hypothesis, if we find an especially good model, then we may make an exception. The $R^2$ values obtained for these three models were as follows:  

\begin{center} \textbf{Figure 2} \end{center}

Model                        $R^2$      $R^2_a$    p-value  
---------------------------  ---------  ---------  ---------  
Accounts ~ Total.Households  0.8303     0.8289     < 2.2e-16  
Accounts ~ Footprint         0.04715    0.03907    0.01721  
Accounts ~ Accts.Hsehld      0.00514    -0.003292  0.4365  

(see __Additional Codeblock 2__ in the _Appendix_ for the code )  
Considering the results of the correlation matrix above, none of these results are particularly surprising. The total number of households in a region proves to be a good predictor of the number of accounts in the region. The coefficient of determination, denoted $R^2$, is high, so we know that a significant amount of the variation in `Accounts` is explained by `Total.Households`. Additionally, the p-value for the coefficient and the model are significant. However, this model only serves to highlight the drawbacks of choosing `Accounts` as the response variable since we can only determine that regions with more households have more accounts. We can say nothing about how likely a given household is to have an account.  
The other two models, which use `Footprint` and `Accts.Hsehld`, are exceptionally poor. `Footprint` is the feature that most interests us since it tells us if a physical bank is in the region. We would have been surprised to see the presence of a physical bank be a good predictor of the gross number of accounts, so seeing that it is not fits in with what we would expect. Now we will look at SLR models predicting the number of accounts per household in a region.  

### Predicting Accounts per Household
Now we look at how each of the individual potential predictors does with `Accts.Hsehld` as the response variable. By using this response variable, we can easily understand which potential predictors are involved in determining the likelihood of a given household having an account with the bank. The $R^2$ values obtained for these three models were as follows:  

\begin{center} \textbf{Figure 3} \end{center}

Model                            $R^2$     $R^2_a$     p-value  
-------------------------------  --------  ----------  -------  
Accts.Hsehld ~ Total.Households  0.007543  -0.0008672  0.3456  
Accts.Hsehld ~ Accounts          0.00514   -0.003292   0.4365  
Accts.Hsehld ~ Footprint         0.02321   0.01493     0.09672  

(see __Additional Codeblock 3__ in the _Appendix_ for the code ) 
Once again, these results are aligned with those from the correlation matrix. All three SLR models are quite poor. Their respective $R^2$ values are very low, and their respective p-values are high. With a higher p-value we cannot reject the null hypothesis that these models are just as good as the average at predicting `Accts.Hsehld`. Interestingly, the $R^2$ and p-values are best, albeit still terrible, for the SLR that used `Footprint`. Depending on what we find when looking at MLR models, Nils Baker's hypothesis still has a chance at being correct.  
Now we will turn our attention to MLR models to try to find one that does a decent job at predicting `Accts.Hsehld`. 

## MLR Models

We feel that the `Accts.Hsheld` is a more appropriate response variable than `Accounts` since it is a ratio of total accounts to the total number of households in that area. Thus, we explore MLR models with `Accts.Hsheld` initially.  

### Models with Accts.Hsheld as response variable
#### Initial Considerations
MLR models can get quite complicated. We can look at how different terms interact with each other, quadratic terms, etc. It is imperative that we keep Nils Barker's hypothesis in mind as we look at MLR models. We want to see how well `Footprint` predicts `Accts.Hsehld`. Thus, we will focus on generating models that will help us answer that question.  

#### Ramsey RESET Test
The Ramsey RESET Test attempts to determine if quadratic terms add value to a model. It does not take into consideration the interaction between quadratic terms and other variables, so this will only help to give us an intuition. The results of the test are below.  

\begin{center} \textbf{Figure 4} \end{center}

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(lmtest)
resettest(lm(Accts.Hsehld ~ Accounts + Total.Households, data = d), power = 2)
resettest(lm(Accts.Hsehld ~ Accounts + Total.Households + Footprint, data = d),
          power = 2)
```  
We see that the p-value for the first RESET test is about as bad as is possible at 0.9965. The second is not much better. This test was run just for possible quadratic terms. We will not evaluate any higher order terms with such a poor value. We did not run the test with `Footprint` in the model initially because a qualitative variable cannot be squared. Thus, we expect that quadratic terms will be most valuable for their interactions, if at all. Now we turn our attention to actual MLR models.  

#### Footprint with one other term
In the correlation matrix (__Figure 1__ above) we saw that `Accounts` and `Total.Households` have a very high correlation (0.911). Thus, we will explore models with only one or the other.  
The code and summaries for each of the models explored can be found in __Additional Codeblock 4__ in the _Appendix_. However, they will not be displayed here because none offered promising results. The $R^2$ value only goes up to 0.119. When $R^2$ is adjusted to account for the number of predictors, $R^2_a$ only gets up to a miniscule 0.08036 . With these dispiriting results we turn our attention to MLR models with both `Accounts` and `Total.Households`.  

#### Using all potential predictors
We know that error cannot be unexplained. Thus, we can look at the $R^2$ value with all possible quadratic and interaction terms to get a sense of just how high we can hope to get that value when building our model and then start to delete variables that are insignificant. Additionally, we can compare $R^2_a$ to make sure that our chosen model improves on this. See __Additional Codeblock 5 and 6__ in the _Appendix_ for the full summaries.  
For the model `Accts.Hsehld ~ (. - ID + I(Accounts^2) + I(Total.Households^2))^2` we get $R^2 = 0.6141$ and $R^2_a = 0.5585$. On comparing this model with a less bloated model,
`Accts.Hsehld ~ . - ID + Footprint:Accounts + Total.Households:Footprint + I(Accounts^2) + I(Total.Households^2) + Footprint:I(Accounts^2) + Footprint:I(Total.Households^2)`, we find all coefficients are significant at the $\alpha = 0.05$ level. That is, we can be at least 95% confident in the significance of these coefficients. $R^2 = 0.5706$ and $R^2_a = 0.5354$. These values are close to the obese one above. Also, the interaction terms with `Footprint` are all the most significant ones.  
Finally, we turn to a peculiar model that highlights the problem with having a response variable built off of two of its potential predictors. When the model is `log(Accts.Hsehld) ~ log(Accounts) + log(Total.Households)`, or any variation that contains those three elements, all variation is explained and $R^2 = R^2_a = 1$. This becomes obvious when looking at the math below.  

\begin{center} \textbf{Proof} \end{center}

Let $A = Accounts$ and $H = Total.Households$.  
Thus $Accts.Hsehld = \frac{A}{H}$.  
Then, $\log{\frac{A}{H}} = b_0 + b_1 \log{A} + b_2 \log{H} \iff \log{A} - \log{H} = b_0 + b_1 \log{A} + b_2 \log{H}$.  
If we let $b_0 = 0$, $b_1 = 1$, $b_2 = -1$, then both sides are equal.  
Thus, we know we know both sides of the regression equation will always be equal. $\Box$  

This highlights the shortcoming of this approach and must cause us to reconsider it when we only have these three potential predictors.  
The low $R^2$ values for the other models force us to reconsider our response variable too. We have seen previously that `Accounts` regressed on `Total.Households` gives the best $R^2$ value so far of 0.8303. Thus, we further explored models with `Accounts` as the response variable.  

### Models with Accounts as response variable
#### Ramsey RESET Test
We first check if there is a possibility of a higher oder term with the Ramsey Test.  

\begin{center} \textbf{Figure 5} \end{center}

```{r echo=FALSE, warning=FALSE, message=FALSE}
resettest(lm(Accounts ~ Total.Households + Footprint, data = d), power = 2)
resettest(lm(Accounts ~ Total.Households + Footprint, data = d), power = 3)
```  

The p-value of 0.03732 obtained from the test definately indicates the presence of second degree term and a p-value of 0.04909 obtained for third degree term also indicates that an even higher term might be present. 

#### Exploring all models

We try and explore as many models as possible (see __Additional Codeblock 8__ in the _Appendix_ for the summary) with degree 2 and degree 3 terms and obtain the following results for $R^2$ and $R^2_a$:  

\begin{center} \textbf{Figure 6} \end{center}

Model                                              $R^2$     $Adjusted R^2$  
------------------------------------------------  -------  -------------------
  Accounts ~ Total.Households + Footprint                   
                  + I(Total.Households^2)          0.8415      0.8374 
  Accounts ~ Total.Households + Footprint                 
               +Total.Households:Footprint         0.8923      0.8895 
  Accounts ~ Total.Households + Footprint       
               + I(Total.Households^2)           
               + Total.Households:Footprint        0.8934      0.8897 
  Accounts ~ Total.Households + Footprint       
               + I(Total.Households^2)           
               + I(Total.Households^3)           
               + Total.Households:Footprint        0.8957      0.8911 
  Accounts ~ Total.Households + Footprint       
               + I(Total.Households^2)           
               + I(Total.Households^2):Footprint   0.8882      0.8843    
  Accounts ~ Total.Households + Footprint         
               + I(Total.Households^2)           
               + Total.Households:Footprint     
               + I(Total.Households^2):Footprint   0.8934      0.8887

               
Looking at the values for all these models we find `Accounts ~ Total.Households + Footprint + Total.Households:Footprint` as the most parsemonius model with high $R^2$ and $R^2_a$ values. Thus we choose it to be the best model that can explain the data. There are some common problems with all the models listed in table 3 as the they are not normal and the residual plots do not look very convincing for any of these. However, with the given amount of data this is the best we could come up with. 

## Results

The best model that we could generate from the given data was `Accounts ~ Total.Households + Footprint + Total.Households:Footprint`. On plotting the residuals and looking for normality of the data we find that the data is not normal but error terms do not show nearly constant variance:  

\begin{center} \textbf{Figure 7} \end{center}

```{r echo=FALSE}
library(car)
residualPlots(lm(Accounts ~ Total.Households + Footprint + 
                  Total.Households:Footprint, data = d))
```  

\begin{center} \textbf{Figure 8} \end{center}

```{r echo=FALSE}
qqPlot(residuals(lm(Accounts ~ Total.Households + Footprint + 
                  Total.Households:Footprint, data = d)), distribution = "norm")
```

The variance of the error terms is further confirmed by using the Breusch-Pagan test which results in the conclusion that variance of the error terms is constant.  

\begin{center} \textbf{Figure 9} \end{center}

```{r}
ncvTest(lm(Accounts ~ Total.Households + Footprint + 
                  Total.Households:Footprint, data = d))
qchisq(.95, 1)
```

The relatively low p - value of the footprint variable is what catches our attention and has the key role in determinig if having a physical branch motivates people to open checking accounts with the bank.

Details how we arrived at the models that we tried and how they worked. Include details about both the processes and the model to which they led. Our thought processes should be detailed so that there is no question how we got to our models. This needs to do all of the leg work so that the Conclusions section can focus on the actual meaning of the model. _This section should include subsections for each model with two hashes and then further subsectioniong using three hashes for each part of the model discussion._  


# Conclusions
The goal is that everything is built up to this point so that little we can just plow right into the meaning of the model. Other general conclusions can be included.  

# Appendix
## Additional Codeblocks

\begin{center} \textbf{Additional Codeblock 1} \end{center}

```{r eval=FALSE}
d <- read.csv("41330723.csv", header = TRUE, stringsAsFactors = FALSE)
d <- d[1:120, ] # last two rows contain no data
names(d) <- c("ID", "Total.Households", "Accounts", "Footprint")

for (i in 2:3) {
  d[[i]] <- gsub(",", "", d[[i]])
  d[[i]] <- as.numeric(d[[i]])
}
d[["Footprint"]][d[["Footprint"]] == "Outside"] <- 0
d[["Footprint"]][d[["Footprint"]] == "Inside"] <- 1
d[["Footprint"]] <- as.numeric(d[["Footprint"]])
d[["ID"]] <- as.numeric(d[["ID"]])
d[["Accts.Hsehld"]] <- d[["Accounts"]] / d[["Total.Households"]]
```  
```{r}
head(d)
```  


\begin{center} \textbf{Additional Codeblock 2} \end{center}

```{r}
lm_1 <- lm(Accounts ~ Total.Households, data = d)
summary(lm_1)

lm_2 <- lm(Accounts ~ Footprint, data = d)
summary(lm_2)

lm_3 <- lm(Accounts ~ Accts.Hsehld, data = d)
summary(lm_3)
```  


\begin{center} \textbf{Additional Codeblock 3} \end{center}
```{r}
lm_4 <- lm(Accts.Hsehld ~ Total.Households, data = d)
summary(lm_4)

lm_5 <- lm(Accts.Hsehld ~ Accounts, data = d)
summary(lm_5)

lm_6 <- lm(Accts.Hsehld ~ Footprint, data = d)
summary(lm_6)
```  


\begin{center} \textbf{Additional Codeblock 4} \end{center}
```{r}
summary(lm(Accts.Hsehld ~ Footprint + Total.Households, data = d))
summary(lm(Accts.Hsehld ~ Footprint + Total.Households +
             Footprint:Total.Households, data = d))

summary(lm(Accts.Hsehld ~ Footprint + Total.Households +
             Footprint:Total.Households + I(Total.Households^2),
             data = d))
summary(lm(Accts.Hsehld ~ Footprint + Total.Households +
             Footprint:Total.Households + I(Total.Households^2) +
             Footprint:I(Total.Households^2),
           data = d))

summary(lm(Accts.Hsehld ~ Footprint + Accounts, data = d))
summary(lm(Accts.Hsehld ~ Footprint + Accounts +
             Footprint:Accounts, data = d))
summary(lm(Accts.Hsehld ~ Footprint + Accounts +
             Footprint:Accounts + I(Accounts^2),
             data = d))
summary(lm(Accts.Hsehld ~ Footprint + Accounts +
             Footprint:Accounts + I(Accounts^2) +
             Footprint:I(Accounts^2),
           data = d))
```  

\begin{center} \textbf{Additional Codeblock 5} \end{center}

```{r}
lm_all <- lm(Accts.Hsehld ~ (. - ID + I(Accounts^2) + I(Total.Households^2))^2,
             data = d)
summary(lm_all)
```  

\begin{center} \textbf{Additional Codeblock 6} \end{center}

```{r}
summary(lm(Accts.Hsehld ~ . - ID + Footprint:Accounts +
             Total.Households:Footprint + I(Accounts^2) +
             I(Total.Households^2) + Footprint:I(Accounts^2) +
             Footprint:I(Total.Households^2),
           data = d))
```  

\begin{center} \textbf{Additional Codeblock 7} \end{center}

```{r}
summary(lm(log(Accts.Hsehld) ~ log(Accounts) + log(Total.Households),
           data = d))
```  

\begin{center} \textbf{Additional Codeblock 8} \end{center}

```{r}
lm_7 <- lm(Accounts ~ Total.Households + Footprint, data = d)
summary(lm_7)

lm_8 <- lm(Accounts ~ Total.Households + Footprint + I(Total.Households^2),
           data = d)
summary(lm_8)

lm_9 <- lm(Accounts ~ Total.Households + Footprint +
           Total.Households:Footprint, data = d)
summary(lm_9)

lm_10 <- lm(Accounts ~ Total.Households + Footprint + I(Total.Households^2) +
             Total.Households:Footprint, data = d)
summary(lm_10)

lm_11 <- lm(Accounts ~ Total.Households + Footprint + I(Total.Households^2) +
             I(Total.Households^3) + Total.Households:Footprint, data = d)
summary(lm_11)

lm_12 <- lm(Accounts ~ Total.Households + Footprint + I(Total.Households^2) +
             I(Total.Households^2):Footprint, data = d)
summary(lm_12)

lm_13 <- lm(Accounts ~ Total.Households + Footprint + I(Total.Households^2) +
             Total.Households:Footprint + I(Total.Households^2):Footprint,
            data = d)
summary(lm_13)
```
## Additional Figures

\begin{center} \textbf{Additional Figure 1} \end{center}

```{r echo=FALSE}
pairs(d[2:length(d)], main = "Pairs Plots for Nils Baker Data")
```  

